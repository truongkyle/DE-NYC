# Content:

* H∆∞·ªõng d·∫´n thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng conda,
* M√¥ t·∫£ chi ti·∫øt t·ª´ng t·∫ßng Bronze / Silver / Gold,
* Gi·∫£i th√≠ch ch·ª©c nƒÉng t·ª´ng container,
* Pipeline th·ª±c thi t·ª´ t·∫£i d·ªØ li·ªáu ‚Üí upload ‚Üí ETL ‚Üí BI.

---

# üöñ NYC Taxi Data Lakehouse ‚Äî Batch Processing Pipeline

> **M·ª•c ti√™u:** X√¢y d·ª±ng h·ªá th·ªëng x·ª≠ l√Ω d·ªØ li·ªáu **batch-oriented data lakehouse** theo ki·∫øn tr√∫c **Medallion (Bronze ‚Üí Silver ‚Üí Gold)**
> v·ªõi c√°c c√¥ng ngh·ªá: **Spark, Delta Lake, Hive Metastore, Trino, MinIO, v√† Metabase.**

---

## üß© Th√†nh ph·∫ßn h·ªá th·ªëng

| Th√†nh ph·∫ßn              | Ch·ª©c nƒÉng                      | C√¥ng ngh·ªá                         |
| ----------------------- | ------------------------------ | --------------------------------- |
| **Data Source**         | NYC Taxi Public Data (Parquet) | nyc.gov TLC datasets              |
| **Ingestion Scripts**   | T·∫£i & upload d·ªØ li·ªáu           | Python (`requests`, `boto3`)      |
| **Storage Layer**       | L∆∞u tr·ªØ file d·ªØ li·ªáu           | MinIO (S3-compatible)             |
| **Processing Layer**    | ETL batch                      | Apache Spark + Delta Lake         |
| **Metadata Layer**      | Qu·∫£n l√Ω schema/table           | Hive Metastore (Postgres backend) |
| **Query Layer**         | Truy v·∫•n d·ªØ li·ªáu b·∫±ng SQL      | Trino                             |
| **Visualization Layer** | Dashboard & BI                 | Metabase                          |

---

## üìÅ C·∫•u tr√∫c th∆∞ m·ª•c

```
DE-NYC/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                # d·ªØ li·ªáu t·∫£i t·ª´ NYC
‚îÇ   ‚îú‚îÄ‚îÄ bronze/             # output c·ªßa Spark Bronze job
‚îÇ   ‚îú‚îÄ‚îÄ silver/             # output c·ªßa Spark Silver job
‚îÇ   ‚îú‚îÄ‚îÄ gold/               # output c·ªßa Spark Gold job
‚îÇ   ‚îú‚îÄ‚îÄ download_nyc_data.py        # t·∫£i d·ªØ li·ªáu t·ª´ NYC Open Data
‚îÇ   ‚îî‚îÄ‚îÄ upload_data_to_Minio.py     # upload d·ªØ li·ªáu l√™n MinIO
‚îÇ
‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze_nyc.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver_trips.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gold_kpi.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ register_tables.py
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ conf/spark-defaults.conf
‚îÇ
‚îú‚îÄ‚îÄ trino/
‚îÇ   ‚îú‚îÄ‚îÄ catalog/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hive.properties
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ delta.properties
‚îÇ   ‚îî‚îÄ‚îÄ etc/
‚îÇ
‚îú‚îÄ‚îÄ spark-jupyter/
‚îÇ   ‚îî‚îÄ‚îÄ notebooks/
‚îÇ
‚îú‚îÄ‚îÄ metabase-data/
‚îÇ   ‚îî‚îÄ‚îÄ metabase.db        # l∆∞u metadata Metabase (SQLite)
‚îÇ
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng l√†m vi·ªác

### 1Ô∏è‚É£ T·∫°o Conda environment

```bash
conda create -n de_env python=3.10 -y
conda activate de_env
```

### 2Ô∏è‚É£ C√†i ƒë·∫∑t c√°c dependencies

```bash
pip install -r requirements.txt
```


## üß∞ C√°c script Python trong th∆∞ m·ª•c `data/`

### `download_nyc_data.py`

* T·∫£i d·ªØ li·ªáu Parquet t·ª´ NYC Open Data API.
* T·ª± ƒë·ªông t·∫°o th∆∞ m·ª•c `/data/raw/nyc_taxi/` n·∫øu ch∆∞a c√≥.
* Cho ph√©p t·∫£i nhi·ªÅu dataset: `yellow`, `green`, `fhv`, `fhvhv`.

V√≠ d·ª•:

```bash
python data/download_nyc_data.py
```

### `upload_data_to_Minio.py`

* Upload d·ªØ li·ªáu t·ª´ `/data/raw/` l√™n MinIO bucket `datalake/raw/nyc_taxi/`..

```bash
python data/upload_data_to_Minio.py
```

---

## üß± Docker Compose ‚Äî M√¥i tr∆∞·ªùng Data Lakehouse

C√°c container ƒë∆∞·ª£c orchestrated b·∫±ng **Docker Compose**.

| Container            | Ch·ª©c nƒÉng                             | Port           |
| -------------------- | ------------------------------------- | -------------- |
| **minio**            | L∆∞u tr·ªØ file S3                       | `9000`, `9001` |
| **metastore_db**     | PostgreSQL backend cho Hive Metastore | `5433`         |
| **hive-metastore**   | Service qu·∫£n l√Ω metadata (Thrift)     | `9083`         |
| **spark-master**     | Spark master node                     | `7077`, `8084` |
| **spark-worker-1/2** | Worker nodes th·ª±c thi job Spark       | ‚Äî              |
| **trino**            | SQL engine query Delta & Hive         | `8080`         |
| **metabase**         | BI dashboard                          | `3000`         |

---

## üß© Makefile ‚Äî C√°c l·ªánh ƒëi·ªÅu khi·ªÉn ch√≠nh

```makefile
run-all:
	docker compose -f docker-compose.yaml up --build -d

down-rm:
	docker compose down -v

build-spark:
	docker build -t spark-nyc:latest ./spark

JOB ?=nyc_test.py
spark-submit:
	docker exec -it spark-master /opt/spark/bin/spark-submit \
	  --master spark://spark-master:7077 /opt/spark-apps/$(JOB)
```

| L·ªánh                                  | M√¥ t·∫£                           |
| ------------------------------------- | ------------------------------- |
| `make run-all`                        | Kh·ªüi ƒë·ªông to√†n b·ªô h·ªá th·ªëng      |
| `make down-rm`                        | D·ª´ng container & x√≥a volumes    |
| `make build-spark`                    | Build Spark image v·ªõi Delta JAR |
| `make spark-submit JOB=bronze_nyc.py` | Ch·∫°y Spark job c·ª• th·ªÉ           |

---

## üöÄ Quy tr√¨nh ch·∫°y to√†n b·ªô pipeline

### 1Ô∏è‚É£ Kh·ªüi ƒë·ªông h·ªá th·ªëng

```bash
make run-all
```

### 2Ô∏è‚É£ T·∫£i d·ªØ li·ªáu NYC

```bash
python data/download_nyc_data.py
```

### 3Ô∏è‚É£ Upload l√™n MinIO

```bash
python data/upload_data_to_Minio.py
```

### 4Ô∏è‚É£ Ch·∫°y ETL jobs

#### Bronze Layer

```bash
make spark-submit JOB=bronze_nyc.py
```

* ƒê·ªçc file t·ª´ `s3a://datalake/raw/nyc_taxi/`
* Th√™m c·ªôt `year`, `month` t·ª´ t√™n file
* Chu·∫©n h√≥a schema
* Ghi Delta partitioned by (`year`, `month`) v√†o `bronze/`

#### Silver Layer

```bash
make spark-submit JOB=silver_trips.py
```

* Chu·∫©n h√≥a schema gi·ªØa c√°c lo·∫°i taxi (`yellow`, `green`, `fhv`, `fhvhv`)
* L√†m s·∫°ch d·ªØ li·ªáu: lo·∫°i b·ªè null, l·ªçc distance/time b·∫•t h·ª£p l√Ω
* Th√™m c·ªôt: `duration_min`, `pickup_date`, `pickup_hour`
* Ghi Delta partitioned by (`pickup_date`, `service_type`) v√†o `silver/`

#### Gold Layer

```bash
make spark-submit JOB=gold_kpi.py
```

* T·ªïng h·ª£p KPI:

  * **daily_revenue_by_zone**: doanh thu/ng√†y/khu v·ª±c
  * **hourly_demand_by_zone**: nhu c·∫ßu/gi·ªù/khu v·ª±c
* Ghi Delta partitioned by (`pickup_date`, `service_type`) v√†o `gold/`

#### Register Tables

```bash
make spark-submit JOB=register_tables.py
```

* T·∫°o database `nyc_gold`
* ƒêƒÉng k√Ω c√°c b·∫£ng Delta trong Hive Metastore.

---

## üìä K·∫øt n·ªëi & tr·ª±c quan h√≥a v·ªõi Metabase

### 1Ô∏è‚É£ Truy c·∫≠p Metabase

```
http://localhost:3000
```

### 2Ô∏è‚É£ Th√™m k·∫øt n·ªëi Trino

| Field            | Value        |
| ---------------- | ------------ |
| **Display name** | Trino        |
| **Host**         | `trino`      |
| **Port**         | `8080`       |
| **Catalog**      | `hive`       |
| **Schema**       | `nyc_gold`   |
| **Username**     | `trino`      |
| **Password**     | *(ƒë·ªÉ tr·ªëng)* |

### 3Ô∏è‚É£ Vi·∫øt query trong Metabase

**+ New ‚Üí SQL Query ‚Üí Database: Trino**

```sql
SELECT pickup_date, service_type, SUM(revenue) AS total_revenue
FROM hive.nyc_gold.daily_revenue_by_zone
GROUP BY pickup_date, service_type
ORDER BY pickup_date
```

> ‚ö†Ô∏è Kh√¥ng d√πng d·∫•u `;` ·ªü cu·ªëi c√¢u.

### 4Ô∏è‚É£ T·∫°o dashboard

* Ch·ªçn **Visualization ‚Üí Line chart / Bar chart**
* Save ‚Üí **Add to dashboard** ‚Üí ‚ÄúNYC Taxi Analytics‚Äù

---


## üß© C√°c truy v·∫•n SQL tham kh·∫£o

```sql
-- Doanh thu theo ng√†y
SELECT pickup_date, service_type, SUM(revenue) AS total_revenue
FROM hive.nyc_gold.daily_revenue_by_zone
GROUP BY pickup_date, service_type
ORDER BY pickup_date;

-- Nhu c·∫ßu theo gi·ªù
SELECT pickup_hour, service_type, SUM(trips) AS total_trips
FROM hive.nyc_gold.hourly_demand_by_zone
GROUP BY pickup_hour, service_type
ORDER BY pickup_hour;

-- Top 10 khu v·ª±c doanh thu cao nh·∫•t
SELECT pu_location_id, SUM(revenue) AS total_revenue
FROM hive.nyc_gold.daily_revenue_by_zone
GROUP BY pu_location_id
ORDER BY total_revenue DESC
LIMIT 10;
```

---

## üßπ D·ªçn d·∫πp h·ªá th·ªëng

```bash
make down-rm
```

> X√≥a to√†n b·ªô container & volume (bao g·ªìm MinIO data, Postgres metadata, Metabase DB).

---
<!-- 
## üîÆ ƒê·ªãnh h∆∞·ªõng m·ªü r·ªông

| M·ª•c ti√™u                   | H∆∞·ªõng ph√°t tri·ªÉn                                                   |
| -------------------------- | ------------------------------------------------------------------ |
| **Streaming Layer**        | PostgreSQL ‚Üí Debezium ‚Üí Kafka ‚Üí Spark Structured Streaming ‚Üí Delta |
| **Workflow Orchestration** | Airflow ho·∫∑c Dagster                                               |
| **Data Quality**           | Great Expectations cho Silver/Gold                                 |
| **Lineage & Governance**   | OpenMetadata / DataHub t√≠ch h·ª£p Hive & Trino                       |
| **Monitoring**             | Prometheus + Grafana theo d√µi Spark, Trino                         |

--- -->

## üéØ K·∫øt lu·∫≠n

Pipeline n√†y cung c·∫•p m·ªôt n·ªÅn t·∫£ng **Data Lakehouse hi·ªán ƒë·∫°i**, bao g·ªìm:

* **ETL** v·ªõi Spark + Delta Lake
* **Metadata** qua Hive Metastore
* **Query & BI** qua Trino + Metabase

‚Üí Gi√∫p x√¢y d·ª±ng quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu batch **chu·∫©n th·ª±c t·∫ø doanh nghi·ªáp**, d·ªÖ m·ªü r·ªông sang **real-time streaming** ho·∫∑c **machine learning pipeline**.

---
